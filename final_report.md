# Stock Forecasting Using Social Media Sentiment: Final Report

## Problem Description:
Can social media predict stock performance? We want to know if tweets and reddit comments can be an accurate predictor of future stock market returns. We wanted to utilize the data generated by various social media to predict the stock market. We believe this project is important because it will allow us to gauge whether or not social media holds any predictive value within financial markets. If social media can help us predict stock returns then we can implement a profitable trading strategy that incorporates social media data.

## Dataset Description:
### Features and Examples:
Our data consists of three main parts. We have data from twitter, data from reddit, and stock return data from Finnhub. All the data spans from the beginning of July 2020 through the end of September 2020. We also chose to concentrate on five large tech companies. We chose Facebook (FB), Apple (AAPL), Amazon (AMZN), Netflix (NFLX), and Google (GOOG). These five companies are commonly referred to by the acronym “FAANG”. 

For the twitter data we looked at how many times each company was mentioned every day from July through September. Each row of data corresponds to a specific date and company and includes the number of posts on that day referring to one of the five companies. 

The reddit data was mainly scraped from two relevant “subreddits”, which are communities dedicated to a specific topic. We took data from the “stocks” subreddit and the “investing” subreddit. Our original dataset includes every post from each subreddit from the beginning of July to the end of September. This ended up being over 25,000 reddit posts. In addition to the title of each post it also includes how many “upvotes” or likes that post received as well as a timestamp. We then transformed this data by counting the number of posts and upvotes each company received per day. Every row of the transformed reddit dataset corresponds to a date and company and has the total number of posts about the company and the total number of upvotes from those posts. 

Lastly, we downloaded historical stock data for each of these companies and calculated daily returns. The historical stock data includes date, opening price, closing price, and volume. We then merged this data with our twitter and reddit dataset. 

The features in our dataset are for each day and company: number of tweets about a company, number of reddit posts about a company, and number of upvotes those reddit posts received. The examples in our dataset are the daily returns for each company. 

### Data Scraping:
**Reddit:** In order to scrape the reddit data we used a combination of the Reddit API and a custom function that is part of the PushShift API in order to download posts from any time period. Since these APIs only allow downloading 100 posts at a time we ran a loop that downloaded posts in one hour time periods. We also added some error handling to the function so deleted posts would not cause the download to break. Overall, we scraped over 25,000 posts and it took over 10 hours of continuous scraping to download all the posts. 

**Twitter:** The Twitter data was scraped through the Twitter Search API that allows you to search past tweets on Twitter through a query. Initially, the API only allows the user to search tweets going back to seven days. You can only query the whole archive of tweets through the premium package, which is what we did. Next, while dealing with a limited number of API requests for Twitter, we decided to query the number of posts that contains each of the five companies’ ticker symbols for each day from July to October.

**Stock Quotes:** In order to get stock quotes of the 5 companies from July to October, we used the Finnhub API, which is a verified financial data API source offering 25 years of historical data of the US market. From the API, we fetched each day’s open and closing price and the volume traded.  

### Data Cleaning:
After scraping the reddit and twitter data we needed to clean it and then merge it to the historical stock data. To clean the reddit data we first had to categorize each post by company. We did this with many hot encoding. We added a column to the reddit posts dataframe for each company and set it to 1 if that post was relevant to that company and 0 otherwise. Notably there were many posts that were not relevant to any of the five companies we are focusing on. After categorizing the posts we aggregated them by date and company to get the final data frame containing the number of posts and upvotes per company per day. We also removed weekends and days with no trading. We then merged both the reddit and twitter data with the historical stock data. 

## Data Analysis:
### Descriptive Overview:
After constructing our dataset we looked at a few basic descriptive statistics. We found that on average from the subreddits we scraped there are only about 2 posts per day on each company. The most talked about company by far was Apple with over 7 posts per day. Netflix, Facebook, and Google were posted about significantly less each averaging around 1 post per day. Another interesting aspect of our data that we found was that each company has a similar number of upvotes per post. Posts on Google received around 36 upvotes on average which was the smallest of the five companies. Posts about Apple and Amazon received on average around 41 upvotes while posts on Facebook and Netflix received on average 52 and 55 upvotes per post. Looking at the twitter data we found that people were far more likely to tweet about Facebook and Amazon stock than Apple, Netflix, or Google stock. We also found that our twitter data for each company has a fairly wide range. Tweets about Facebook ranged from under 300,000 to over 750,000 per day. 

### Data Visualization
We plotted scatter plots below for each of the companies in FAANG to demonstrate the relationship between the number of Twitter posts containing the company’s ticker symbol and the daily returns of each company’s stock. Plotting these points, we calculated that the average correlation between the stock’s daily return and the number of posts is approximately 0.0527 with Netflix containing the strongest relationship at a correlation of 0.1407. There seems to be a very weak linear relationship between twitter posts and stock daily returns.

<p align = "center">
  <img src = "/Plots/plots1.png">
  <img src = "/Plots/plots2.png">
</p>

In addition, we examined the relationship between daily returns and the daily counts of each social media posts. First, as shown in the hisogram below, the daily return of a stock price mostly ranges from -5% to 5%. Next, We hypothesized that if there are more posts on social media, the absolute value of the daily returns would increase under the assumption that when a stock price changes greatly, people would talk about it more. However, as shown in the plots below, the red and ble point are disperesed in a disordered fashion, indicating that classifying a positive return and a negative return only using the post counts seems to be nearly impossible.

<p align = "center">
  <img src = "/Plots/download-6.png">
</p>

## Model Construction:
### Linear Regression Model:
First, we try to model the FAANG returns with linear regression, involving different feature transformations. We’ll create 5 separate linear models that attempt to predict the returns of each company in FAANG. We’ll create a new feature that tells us the tweet increases or decreases per day that involves the ticker symbol for each company on Twitter. With this, we established a training data and a testing data that is not randomly shuffled. When fitting the data for each FAANG company into linear regression models, as expected we get weak results. We get negative R-squared values of -0.571, -0.112, and -0.371 for Facebook, Apple, and Google respectively. A negative R-squared value simply means that the regression line performs worse than the mean of the data, thus making it an extremely weak model. For Amazon and Netflix, we get small R-squared values that again makes the model weak. 

Next, we’ll try to add polynomial transformation to our model. When putting each company’s data through polynomial regression, we witness very low mean squared error values for each company, which most likely means the polynomial results in overfitting. We’ve adjusted the polynomial model through various degrees and most of the models result in overfitting. As we look at the models below, we see that the randomness of our data makes our polynomial model difficult to fit. The presence of outliers in the data has a deep effect of the outcome we receive in the predicted returns. Polynomial regressions are not resistant to the hyper volatility that occurs frequently on Twitter.

<p align = "center">
  <img src = "/Plots/FAAN.png" width="500" height="500" />
  <img src = "/Plots/Google.png" width="300" height="300" />
</p>

Finally, we dig deeper into the feature engineering of our model by including Reddit into our model. We’ll multiply the tweet activity and reddit activity to form one column to use in a linear regression model again. This results in a linear regression model that is similar to the first linear regression model that was developed. All the R-squared values that were produced are all negative for each company, implying no relationship between the two social media sites. Again, it shows that linear regression isn’t the appropriate model to use to predict stock returns based on the data from Twitter and Reddit.

### Classification Model:

After learning that our linear regression model could not get an accurate predication model, we decided to create a forecasting classification model to predict whether the daily return would be positive or negative. Ultimately, our y for this model would be a boolean which is true if the return is positive and false if the return is negative.

The following is a simple classification model with minimal feature transformations. For this model we only used the features of the number of tweets, the number of reddit posts, and the number of upvotes. After randomly shuffling the data, we split it into training and testing sets using 60% of the data for training and 40% for testing. We then fit a logistic regression with a quadratic regularizer to the data. We chose logistic loss since it is well suited for binary classification problems like this one. This simple model ended up being not particularly effective and gave us a misclassification error of 47% on our test set. Furthermore, the model did not make “distinct” predictions and instead learned to always predict “positive”.

Following the sub-par performance of this model, we decided to break our data setup and fit a model for each company. We did this since the range of values for tweets, reddit posts, and upvotes for each company were fairly different, which may explain why the model performed so poorly before. Once again we used logistic loss and a quadratic regularizer for each model. Our models did not perform well on the test set and had similar results as our initial approach. However, the model fit to Apple returns was an exception. This model produced predictions that were not homogenous and had a misclassification error of only 38%. Based on this, we decided to focus more on building a model for Apple stock returns since our initial apple model showed promising results. First, we scraped additional data from the apple subreddit and combined that data with our existing twitter data. Based on this data, we fit another model using the same parameters and achieved similar results. Again our misclassification error was only 38%. The weights of this model indicate that “Number of Reddit Posts” was the most important feature in our Apple Returns model. Despite the low misclassification rate and promising results, we are still not very confident in this model. First, we only had data for 64 trading days. Scraping data from reddit is an extremely slow process and this severely limited the number of days we were able to use. Second, the weights in our model are all relatively small, suggesting our features are not very predictive of stock returns. Perhaps if we had a year's worth of data or more, we would be able to make better predictions. It is also likely that this model does provide some insight, however by itself it is not particularly useful. Our more general model and models fit to other stocks performed no better than guessing and should definitely not be used. One reason Apple returns may be more correlated to Twitter and Reddit sentiment is due to Apple being talked about on these platforms far more than other stocks and Apple’s popularity as a stock. The other stocks we looked at are generally not as popular among “retail” investors and are mentioned less frequently online than Apple. Below is a summary of the performance of the various models:

<p align = "center">
  <img src = "/Plots/modelbargraph.png">
</p>

### Further Exploration with Classifier:

As we believed our logistic loss classification model was not enough, we came up with another strategy to our classifier model. As stock investors personally, we believed that if we were to truly utilize our model to optimize our investment, instead of shuffling the dataset and splitting it to training and testing parts randomly, we would use the data split up the data in a chronological fashion. Thus, for our new model, we decided that our training data set would be the data set from August, and our testing data set would be the data set from September. Unlike the model mentioned previously, we also included open and closing price point, volume of trade for the day as features.
Instead of using logistic loss, we decided to use hinge loss and quadratic regularizer on our data. Hinge loss adds better accuracy and more sparsity to our model, compared to logistic loss. Initially, we generated a classifier model on the entire dataset and the misclassification error was roughly 0.5048, which we thought was better than we expected as the goal of this model was to produce a model that has an error rate of less than 50%. Then, we did a time series lag on the model using 1, 5, 10, 15, 20 days. The misclassification error of each day turned out to be 0.5048, 0.5381, 0.4524, 0.4286, 0.4091. As displayed by the number, it is currently evident that the model generally classifies better, as we take into account more prior days of the data. Then we performed the same method but with time series lags with 1, 5, 10, 15, 20 days, and, in addition, we decided to create a model separately for each companies as well. 

<p align = "center">
  <img width = "500" src = "/Plots/whole.png">
</p>

As shown by the plot above, the model performs generally well. As we increase the number of days used for our times series lags, the misclassification error decreased. However, as we modeled this using the entire dataset, this does not give an accurate estimation of the model. Thus, as mentioned above, we split our data into August and September, let our model learn on our August data and tested the model using the September data. Our result is shown below. 

<p align = "center">
  <img width = "500" src = "/Plots/train.png">
  <img width = "500" src = "/Plots/test.png">
</p>

## Weapon of Math Destruction:
We do not believe our project has created a "Weapon of Math Destruction" since our models do not fit the three main criteria of a "Weapon of Math Destruction". 
1. Our model has easily measurable outcomes. The goal of our model is not ambigous and we are able to directly measure our results.
2. The predictions given by our model do not have negative or unintended consequences. Our predictions do not affect the well being of others or the health of any of the companies we performed analysis on. 
3. Our models do not create feedback loops. The output of our model does not have any affect on the inputs and therefore we are not worried about creating a feedback loop. 





